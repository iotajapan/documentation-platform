{"path":"docs/iri/0.1/how-to-guides/flink-tangle-stream-processing","templateID":1,"sharedPropsHashes":{},"localProps":{"markdown":"# Process ZMQ events in near real-time with Apache Flink\n\n**When you subscribe to ZMQ events, you receive near real-time Tangle data from a node. To process this data, you can use a stream processing framework such as the open-source [Apache Flink](https://flink.apache.org/).**\n\nThis guide uses the [Flink Tangle source library](https://github.com/Citrullin/flink-tangle-source) to use process ZMQ data with Flink. \n\nThis library uses the [ZeroMQMessageParser](https://github.com/Citrullin/tangle-streaming/blob/master/src/main/scala/org/iota/tangle/stream/ZeroMQMessageParser.scala) from the [Tangle streaming library](https://github.com/Citrullin/tangle-streaming) to parse the raw event messages into class instances.\nAll ZMQ event messages are wrapped in classes that are generated by [protobuf schema files](https://github.com/Citrullin/tangle-streaming/tree/master/src/main/protobuf). All protobuf messages and attributes are also available in Flink.\n\nBecause this library uses the ZMQ API, all [ZMQ events](../references/zmq-events.md) are available for processing.\n\n:::info:\nThe Tangle streaming libraries in this guide are not recommended for production environments.\nFeel free to contribute to the libraries, so that they eventually become production ready.\n:::\n\n## Prerequisites\n\nTo complete this guide, you need the following:\n\n- **Operating system:** Linux, MacOS, BSD or Windows\n- **RAM:** 2GB\n- **Storage:** 10GB free space\n\nThis guide uses the Scala programming language with the sbt build tool.\n\nIf you want to use Scala in a Java Runtime Environment (JRE), you need to add the Scala library to [Maven](https://mvnrepository.com/artifact/org.scala-lang/scala-library) or [sbt](http://xerial.org/blog/2014/03/24/sbt/).\n \nThis [Artima guide](https://www.artima.com/pins1ed/combining-scala-and-java.html) describes how you can use Scala in a JRE.\n\n## Download and install the libraries\n\n1. [Install Java](http://openjdk.java.net/install/). Because Scala uses the Java virtual machine, you must install Java 8 or higher.\n\n2. [Install sbt](https://www.scala-sbt.org/1.x/docs/Setup.html)\n\n3. Clone the libraries\n\n  ```bash\n  git clone https://github.com/Citrullin/tangle-streaming.git\n  git clone https://github.com/Citrullin/flink-tangle-source\n  ```\n\n4. Change into the `tangle-streaming` directory and initialize the REPL (Read-Evaluate-Print Loop)\n\n  ```bash\n  cd tangle-streaming && sbt\n  ```\n\n5. In the REPL, build the library\n\n  ```bash\n  compile\n  publishLocal\n  ```\n\n6. Press **Ctrl** + **C** to terminate the REPL\n\n7. Change into the `flink-tangle-source` directory and initialize the REPL\n\n  ```bash\n  cd ../flink-tangle-source && sbt\n  ```\n\n8. In the REPL, build the library\n\n  ```bash\n  compile\n  publishLocal\n  ```\n\n9. Add the dependencies to the `build.sbt` file\n\n  ```scala\n  libraryDependencies += \"org.iota\" %% \"flink-tangle-source\" % \"0.0.1\",\n  ```\n\n:::success:\nNow that you've downloaded and installed the libraries you can start using them to process ZMQ data.\nWe have [some examples available here](https://github.com/iota-community/flink-tangle-examples).\n:::\n\n:::info:\nIf you run your own IRI node, you have to [enable the ZMQ configuration parameter](../references/iri-configuration-options.md).\n\n[Tanglebeat provides a list of public nodes that have ZMQ enabled.](http://tanglebeat.com/page/internals).\n\ncIRI does not support the ZMQ API at the moment.\n:::\n\n## Process the top 10 most used addresses in the last hour\n\nYou can use the data in ZMQ event streams to find out the top 10 most used addresses in the last hour.\n\nThis code in this guide is available in the `MostUsedAddresses.scala` file on [this IOTA community GitHub repository](https://github.com/iota-community/flink-tangle-examples).\n\n## Prerequisites\n\nIf you are not familiar with Flink, you should read [this documentation](https://ci.apache.org/projects/flink/flink-docs-master/tutorials/datastream_api.html#writing-a-flink-program).\n\nYou must have [downloaded and installed the libraries](#download-and-install-the-libraries).\n\n---\n\nSet up the stream by connecting to a node\n\n```scala\nval unconfirmedMessageDescriptorName = UnconfirmedTransactionMessage.scalaDescriptor.fullName\nval zeroMQHost = \"HOSTNAME|IP\"\nval zeroMQPort = config.getInt(ConfigurationKeys.ZeroMQ.port)\nval env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n\nval stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, \"\"))\n```\n:::info:\nHere, we connect to a node by its hostname and port. We could subscribe to a specific topic such as the [tx](../references/zmq-events.md#tx) event:\n\n```scala\nval stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, \"tx\"))\n```\n:::\n\nSince we get a stream of GeneratedMessage, we need to filter with the [protobuf descriptor](https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.descriptor). \n\n```scala\nval filteredStream = stream\n  .filter(_.companion.scalaDescriptor.fullName == unconfirmedMessageDescriptorName)\n```\n\nWe can make sure with this that the stream only contains UnconfirmedTransactionMessages.\nSo now we check the type, wrap it in an option and getting the value. \nSince we already filtered on the descriptor, we know that every event is of type UnconfirmedTransactionMessage.\nIf not, something fundamental is wrong and a NullPointerException will crash the application.\n\n```scala\nval unconfirmedTransactionStream = filteredStream.map(_ match {\n        case m: UnconfirmedTransactionMessage => Some(m)\n        case _ => None\n      })\n      .map(_.get)\n```\n\nThis is a uncommon and dirty way to do. You should never use get, since you can run into `NullPointer` exceptions.\nUse [getOrElse](https://www.tutorialspoint.com/scala/scala_options.htm) instead. \nIt would also make sense to implement a filter into the library, so that the correct type is returned.\nThat would make the type checking obsolete.\nSince this library is just a proof of concept, we go with this dirty solution for now.\n\nNow we have our stream of the type UnconfirmedTransactionMessage. We basically get every message our full-node receives.\nWe want to find out which addresses were used the most. That means, we only need the address and some counter.\nFor simplicity we count every address in a transaction as one. We could also only keep the inputs. \nTo detect double used addresses, we can also filter on outputs. \nIf you want to do that, you have to apply a filter with value > 0 or value < 0.\n\n```scala\nval addressOnlyStream = unconfirmedTransactionStream.map(e => (e.address, 1L))\n```\n\nSimple as that. We change the structure of our element with this simple map function.\nWe only keep the address and a counter. [Tuples](https://docs.scala-lang.org/tour/tuples.html) are useful for this.\n\nSince we want to count our elements, we can key our stream by the address. \nThis gives us a KeyedStream partitioned by the address. For more complex use-cases you can use [windowAll](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/windows.html#window-assigners)\n\n```scala\nval keyedStream = addressOnlyStream.keyBy(_._1)\n```\n \nSo, now we have a KeyedStream where every partition only contain some tuples of the same kind. \nBasically a lot of tuples with the same address and 1L.\nPartitioning is useful if you want to process a huge amount of data.\nFlink can execute the processor for each partition on different nodes in your cluster, so that the process functions\non each stream can work in parallel. Therefore you are able to scale horizontally.\n\nNext, we need to calculate the number of transactions for each address within one hour. \nSliding Windows are useful for this. An update interval of 30 seconds is fine for our use-case.\n\n```scala\nval keyedTimedWindow = keyedStream.timeWindow(Time.minutes(60), Time.seconds(30))\n```\n\nWe got our keyedTimeWindows. Now we need to aggregate our partitions. \nWe have two options for this. The simplest variant is the reduce function. \nThis is a function which reduces all elements to the few we really need. \nIn our case, this would be our reduce function:\n\n```scala\nval aggregatedKeyedTimeWindow = timedWindow.reduce((a, b) => (a._1, a._2 + b._2))\n```\n\nSimple as that. The other variant is an aggregation function. One example:\n\n```scala\nval aggregatedKeyedTimeWindow = keyedTimedWindow.aggregate(new AddressCountAggregator)\n```\n\nThe AddressCountAggregator class\n\n```scala\nclass AddressCountAggregator extends AggregateFunction[(String, Long), (String, Long), (String, Long)]\n{\n  override def add(value: (String, Long), accumulator: (String, Long)): (String, Long) =\n    (value._1, value._2 + accumulator._2)\n\n  override def createAccumulator(): (String, Long) = (\"\", 0L)\n\n  override def getResult(accumulator: (String, Long)): (String, Long) = accumulator\n\n  override def merge(a: (String, Long), b: (String, Long)): (String, Long) = (a._1, a._2 + b._2)\n}\n```\n\nThe reduce function is used whenever you just need to reduce your result. Sums are a good example. \nTherefore in our case the reduce function makes more sense than the aggregation function.\nAggregation functions are helpful when you have complex operations. \nYou can find one more complex example in [BundleAggregation.scala](https://github.com/iota-community/flink-tangle-examples/blob/master/src/main/scala/org/iota/tangle/flink/examples/BundleAggregation.scala).\nThe BundleAggregation combines incoming transaction into a Bundle and split them into UnconfirmedBundles\nand ReattachedUnconfirmedBundles. This example is a simplification and does not split the Bundles in an accurate way.\n\nNext we want to aggregate all elements and want to find the top ten addresses.\nThe timeWindowAll functions returns a AllWindowedStream. So all elements are combined in one stream again. \nSince we used a SlidingWindow on our partitions before, the time here is not that important anymore. \nSo, we just use one second.\n\n```scala\nval timeWindowAll = aggregatedKeyedTimeWindow\n      .timeWindowAll(Time.seconds(1))\n```\n\nOur AllWindowedStream contains all reduced partitions in a tuple. \nEach partition has one tuple in the structure (ADDRESS, AMOUNT_OF_TRANSACTIONS).\nThe last step is to find out which addresses are used the most. So we use an aggregation function for this.\n\n```scala\nval mostUsedStream = timeWindowAll.aggregate(new MostUsedAddressesAggregator(10))\n```\n\nThe MostUsedAddressesAggregator class\n\n```scala\nclass MostUsedAddressesAggregator(number: Int) extends AggregateFunction[(String, Long), Map[String, Long], List[(String, Long)]]\n{\n  override def add(value: (String, Long), accumulator: Map[String, Long]): Map[String, Long] = {\n    accumulator ++ Map(value._1 -> (value._2 + accumulator.getOrElse(value._1, 0L)))\n  }\n\n  override def createAccumulator(): Map[String, Long] = Map()\n\n  override def getResult(accumulator: Map[String, Long]): List[(String, Long)] =\n    accumulator.toList.sortWith(_._2 > _._2).take(number)\n\n  override def merge(a: Map[String, Long], b: Map[String, Long]): Map[String, Long] = {\n    val seq = a.toSeq ++ b.toSeq\n    val grouped = seq.groupBy(_._1)\n    val mapWithCounts = grouped.map{case (key, value) => (key, value.map(_._2))}\n\n    mapWithCounts.map{case (key, value) => (key, value.sum)}\n  }\n}\n```\n\nWe use a Map as accumulator. Maps are really useful, since they contain key value pairs. \nAggregateFunction returns a sorted List. From the top used address to the bottom one. \nWe are only interested in the first ten, so we only take the first 10. \nThe constructor of the class takes the number for it.\n\nThe last step is simple, print the List and execute our program.\n\n```scala\nmostUsedStream.print()\n\n    // execute program\n    env.execute(\"Most used addresses\")\n```\n\n","title":"ビッグデータトランザクションストリームをほぼリアルタイムで処理する | ハウツーガイド | IRI"}}
